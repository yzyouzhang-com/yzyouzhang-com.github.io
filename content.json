{"meta":{"title":"You Zhang's Personal Page","subtitle":"","description":"","author":"You Zhang","url":"http://yzyouzhang.com","root":"/"},"pages":[],"posts":[{"title":"Hello World","slug":"hello-world","date":"2020-07-17T20:16:49.000Z","updated":"2020-07-17T20:16:49.000Z","comments":true,"path":"2020/07/17/hello-world/","link":"","permalink":"http://yzyouzhang.com/2020/07/17/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Happy Birthday You","slug":"happy-birthday","date":"2020-07-17T20:16:49.000Z","updated":"2020-07-17T22:21:47.471Z","comments":true,"path":"2020/07/17/happy-birthday/","link":"","permalink":"http://yzyouzhang.com/2020/07/17/happy-birthday/","excerpt":"Happy Birthday，多吃一点长肉肉！以下是一个markdown文档的demo，enjoy！UIS-RNN OverviewThis is the library for theUnbounded Interleaved-State Recurrent Neural Network (UIS-RNN) algorithm.UIS-RNN solves the problem of segmenting and clustering sequential databy learning from examples. This algorithm was originally proposed in the paperFully Supervised Speaker Diarization. The work has been introduced byGoogle AI Blog. DisclaimerThis open source implementation is slightly different than the internal onewhich we used to produce the results in thepaper, due to dependencies onsome internal libraries. We CANNOT share the data, code, or model for the speaker recognition system(d-vector embeddings)used in the paper, since the speaker recognition systemheavily depends on Google’s internal infrastructure and proprietary data. This library is NOT an official Google product. We welcome community contributions (guidelines)to the uisrnn/contrib folder.But we won’t be responsible for the correctness of any community contributions. DependenciesThis library depends on: python 3.5+ numpy 1.15.1 pytorch 1.3.0 scipy 1.1.0 (for evaluation only) Getting Started Install the packageWithout downloading the repository, you can install thepackage by: 1pip3 install uisrnn or 1python3 -m pip install uisrnn Run the demoTo get started, simply run this command: 1python3 demo.py --train_iteration=1000 -l=0.001","text":"Happy Birthday，多吃一点长肉肉！以下是一个markdown文档的demo，enjoy！UIS-RNN OverviewThis is the library for theUnbounded Interleaved-State Recurrent Neural Network (UIS-RNN) algorithm.UIS-RNN solves the problem of segmenting and clustering sequential databy learning from examples. This algorithm was originally proposed in the paperFully Supervised Speaker Diarization. The work has been introduced byGoogle AI Blog. DisclaimerThis open source implementation is slightly different than the internal onewhich we used to produce the results in thepaper, due to dependencies onsome internal libraries. We CANNOT share the data, code, or model for the speaker recognition system(d-vector embeddings)used in the paper, since the speaker recognition systemheavily depends on Google’s internal infrastructure and proprietary data. This library is NOT an official Google product. We welcome community contributions (guidelines)to the uisrnn/contrib folder.But we won’t be responsible for the correctness of any community contributions. DependenciesThis library depends on: python 3.5+ numpy 1.15.1 pytorch 1.3.0 scipy 1.1.0 (for evaluation only) Getting Started Install the packageWithout downloading the repository, you can install thepackage by: 1pip3 install uisrnn or 1python3 -m pip install uisrnn Run the demoTo get started, simply run this command: 1python3 demo.py --train_iteration=1000 -l=0.001 This will train a UIS-RNN model using data/toy_training_data.npz,then store the model on disk, perform inference on data/toy_testing_data.npz,print the inference results, and save the averaged accuracy in a text file. PS. The files under data/ are manually generated toy data,for demonstration purpose only.These data are very simple, so we are supposed to get 100% accuracy on thetesting data. Run the testsYou can also verify the correctness of this library by running: 1bash run_tests.sh If you fork this library and make local changes, be sure to use these testsas a sanity check. Besides, these tests are also great examples for learningthe APIs, especially tests/integration_test.py. Core APIsGlossary General Machine Learning Speaker Diarization Sequence Utterance Observation / Feature Embedding / d-vector Label / Cluster ID Speaker ArgumentsIn your main script, call this function to get the arguments: 1model_args, training_args, inference_args = uisrnn.parse_arguments() Model constructionAll algorithms are implemented as the UISRNN class. First, construct aUISRNN object by: 1model = uisrnn.UISRNN(args) The definitions of the args are described in uisrnn/arguments.py.See model_parser. TrainingNext, train the model by calling the fit() function: 1model.fit(train_sequences, train_cluster_ids, args) The definitions of the args are described in uisrnn/arguments.py.See training_parser. The fit() function accepts two types of input, as described below. Input as list of sequences (recommanded)Here, train_sequences is a list of observation sequences.Each observation sequence is a 2-dim numpy array of type float. The first dimension is the length of this sequence. And the lengthcan vary from one sequence to another. The second dimension is the size of each observation. Thismust be consistent among all sequences. For speaker diarization,the observation could be thed-vector embeddings. train_cluster_ids is also a list, which has the same length astrain_sequences. Each element of train_cluster_ids is a 1-dim list ornumpy array of strings, containing the ground truth labels for thecorresponding sequence in train_sequences.For speaker diarization, these labels are the speaker identifiers for eachobservation. When calling fit() in this way, please be very careful with the argument--enforce_cluster_id_uniqueness. For example, assume: 1train_cluster_ids = [['a', 'b'], ['a', 'c']] If the label &#39;a&#39; from the two sequences refers to the same cluster acrossthe entire dataset, then we should have enforce_cluster_id_uniqueness=False;otherwise, if &#39;a&#39; is only a local indicator to distinguish from &#39;b&#39; in the1st sequence, and to distinguish from &#39;c&#39; in the 2nd sequence, then we shouldhave enforce_cluster_id_uniqueness=True. Also, please note that, when calling fit() in this way, we are going toconcatenate all sequences and all cluster IDs, and delegate tothe next section below. Input as single concatenated sequenceHere, train_sequences should be a single 2-dim numpy array of type float,for the concatenated observation sequences. For example, if you have M training utterances,and each utterance is a sequence of L embeddings. Each embedding isa vector of D numbers. Then the shape of train_sequences is N * D,where N = M * L. train_cluster_ids is a 1-dim list or numpy array of strings, of length N.It is the concatenated ground truth labels of all training data. Since we are concatenating observation sequences, it is important to note that,ground truth labels in train_cluster_id across different sequences aresupposed to be globally unique. For example, if the set of labels in the firstsequence is {&#39;A&#39;, &#39;B&#39;, &#39;C&#39;}, and the set of labels in the second sequenceis {&#39;B&#39;, &#39;C&#39;, &#39;D&#39;}. Then before concatenation, we should rename them tosomething like {&#39;1_A&#39;, &#39;1_B&#39;, &#39;1_C&#39;} and {&#39;2_B&#39;, &#39;2_C&#39;, &#39;2_D&#39;},unless &#39;B&#39; and &#39;C&#39; in the two sequences are meaningfully identical(in speaker diarization, this means they are the same speakers acrossutterances). This part will be automatically taken care of by the argument--enforce_cluster_id_uniqueness for the previous section. The reason we concatenate all training sequences is that, we will be resamplingand block-wise shuffling the training data as a data augmentationprocess, such that we result in a robust model even when there is insufficientnumber of training sequences. Training on large datasetsFor large datasets, the data usually could not be loaded into memory at once.In such cases, the fit() function needs to be called multiple times. Here we provide a few guidelines as our suggestions: Do not feed different datasets into different calls of fit(). Instead,for each call of fit(), the input should cover sequences fromdifferent datasets. For each call to the fit() function, make the size of input roughly thesame. And, don’t make the input size too small. PredictionOnce we are done with training, we can run the trained model to performinference on new sequences by calling the predict() function: 1predicted_cluster_ids = model.predict(test_sequences, args) Here test_sequences should be a list of 2-dim numpy arrays of type float,corresponding to the observation sequences for testing. The returned predicted_cluster_ids is a list of the same size astest_sequences. Each element of predicted_cluster_ids is a list of integers,with the same length as the corresponding test sequence. You can also use a single test sequence for test_sequences. Then the returnedpredicted_cluster_ids will also be a single list of integers. The definitions of the args are described in uisrnn/arguments.py.See inference_parser. CitationsOur paper is cited as: 12345678@inproceedings&#123;zhang2019fully, title&#x3D;&#123;Fully supervised speaker diarization&#125;, author&#x3D;&#123;Zhang, Aonan and Wang, Quan and Zhu, Zhenyao and Paisley, John and Wang, Chong&#125;, booktitle&#x3D;&#123;International Conference on Acoustics, Speech and Signal Processing (ICASSP)&#125;, pages&#x3D;&#123;6301--6305&#125;, year&#x3D;&#123;2019&#125;, organization&#x3D;&#123;IEEE&#125;&#125; ReferencesBaseline diarization systemTo learn more about our baseline diarization system based onunsupervised clustering algorithms, check outthis site. A Python re-implementation of the spectral clustering algorithm used in thispaper is available here. The ground truth labels for theNIST SRE 2000dataset (Disk6 and Disk8) can be foundhere. For more public resources on speaker diarization, check out awesome-diarization. Speaker recognizer/encoderTo learn more about our speaker embedding system, check outthis site. We are aware of several third-party implementations of this work: Resemblyzer: PyTorch implementation by resemble-ai TensorFlow implementation by Janghyun1230 PyTorch implementaion by HarryVolek - with UIS-RNN integration PyTorch implementation as part of SV2TTS Please use your own judgement to decide whether you want to use theseimplementations. We are NOT responsible for the correctness of any third-party implementations. VariantsHere we list the repositories that are based on UIS-RNN, but integrated withother technologies or added some improvements. Link Description taylorlu/Speaker-Diarization Speaker diarization using UIS-RNN and GhostVLAD. An easier way to support openset speakers. DonkeyShot21/uis-rnn-sml A variant of UIS-RNN, for the paper Supervised Online Diarization with Sample Mean Loss for Multi-Domain Data.","categories":[],"tags":[]}],"categories":[],"tags":[]}